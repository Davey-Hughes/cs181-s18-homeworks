
\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{David Hughes}
\email{davidralphhughes@college.harvard.edu}

% List any people you worked with.
\collaborators{%
    Alexander Munoz
}

% You don't need to change these.
\course{CS181-S18}
\assignment{Assignment \#4}
\duedate{11:59pm March 30, 2018} % FDV: Update due date

\usepackage[OT1]{fontenc}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{todonotes}
\usepackage{listings}
\usepackage{common}
\usepackage{bm}

\usepackage[mmddyyyy,hhmmss]{datetime}

\definecolor{verbgray}{gray}{0.9}

\lstnewenvironment{csv}{%
  \lstset{backgroundcolor=\color{verbgray},
  frame=single,
  framerule=0pt,
  basicstyle=\ttfamily,
  columns=fullflexible}}{}

\begin{document}
\begin{center}
{\Large Homework 4: Clustering and EM}\\
\end{center}


This homework assignment focuses on different unsupervised learning
methods from a theoretical and practical standpoint.  In Problem 1,
you will explore Hierarchical Clustering and experiment with how the
choice of distance metrics can alter the behavior of the algorithm. In
Problem 2, you will derive from scratch the full
expectation-maximization algorithm for fitting a Gaussian mixture
model. In Problem 3, you will implement K-Means clustering on a
dataset of handwritten images and analyze the latent structure learned
by this algorithm.

There is a mathematical component and a programming component to this
homework.  Please submit your PDF and Python files to Canvas, and push
all of your work to your GitHub repository. If a question requires you
to make any plots, please include those in the writeup.



\newpage
\section*{Hierarchical Clustering [7 pts]}

At each step of hierarchical clustering, the two most similar clusters
are merged together. This step is repeated until there is one single
group. We saw in class that hierarchical clustering will return a
different result based on the pointwise-distance and cluster-distance
that is is used. In this problem you will examine different choices of
pointwise distance (specified through choice of norm) and cluster
distance, and explore how these choices change how the HAC algorithm
runs on a toy data set.


\vspace{0.25cm}

\begin{problem}
~

 Consider the following four data points in $\reals^2$, belonging to three clusters: the
  black cluster consisting of $\boldx_1 = (0.1, 0.5) $ and $\boldx_2 = (0.35, 0.75))$,
  the red cluster consisting of $\boldx_3 = (0.28, 1.35)$, and the blue cluster
  consisting of $\boldx_4 = (0, 1.01)$.

  \begin{center} \includegraphics[scale=.3]{scatterplot.png} \end{center}


  Different pointwise distances $d(\boldx, \boldx') = \|\boldx - \boldx'\|_p$
  can be used.  Recall the definition of the
  $\ell_1$, $\ell_2$, and $\ell_{\infty}$ norm:
  \begin{eqnarray*}
     \| \mathbf{x} \|_1 = \sum_{j = 1}^m |x_i| \quad \quad\quad \| \mathbf{x} \|_2 = \sqrt{\sum_{j = 1}^m x_i^2 } \quad\quad\quad
     \| \mathbf{x} \|_{\infty} = \max_{j \in \{1, \ldots, m\}} |x_j|\\
  \end{eqnarray*}

  Also recall the definition of min-distance, max-distance,
  centroid-distance, and average-distance between two clusters (where $\bmu_{G}$
is the center of a cluster $G$):
%
\begin{eqnarray*}
    d_{\text{min}}(G, G') &=& \min_{\boldx  \in G, \boldx' \in G'} d(\boldx, \boldx')\\
    d_{\text{max}}(G, G') &=& \max_{\boldx  \in G, \boldx' \in G'} d(\boldx, \boldx')\\
    d_{\text{centroid}}(G, G') &=&  d(\bmu_{G}, \bmu_{G'})\\
    d_{\text{avg}}(G, G') &=&\frac{1}{|G| |G'|} \sum_{\boldx \in G}\sum_{\boldx'  \in G'} d(\boldx, \boldx')\\
  \end{eqnarray*}

  \begin{enumerate}
    \item Draw the 2D unit sphere for each norm, defined as $\mathcal{S} =
        \{\boldx \in \mathbb{R}^2: \|\boldx\| = 1 \}$. Feel free to do it by
        hand, take a picture and include it in your pdf.
    \item  For each norm ($\ell_1, \ell_2, \ell_\infty$) and each clustering
        distance, specify which two clusters would be the first to merge.
    \item Draw the complete dendrograms showing the order of agglomerations for
        the $\ell_2$ norm and each of the clustering distances. We have
        provided some code to make this easier for you. You are not required to
        use it.
  \end{enumerate}

\end{problem}

\subsection*{Solution}
\begin{enumerate}
    \item \text{} \\
        \begin{tikzpicture}[scale=0.5]
            \draw [<->] (-4,0)--(4,0);
            \draw [<->] (0,-4)--(0,4);
            \draw [blue] (-3,0) -- (0,3) -- (3,0) -- (0,-3) -- (-3,0);
            \node at (0, -4.5) {$\ell_1$};
        \end{tikzpicture}

        \begin{tikzpicture}[scale=0.5]
            \draw [<->] (-4,0)--(4,0);
            \draw [<->] (0,-4)--(0,4);
            \draw [blue] (0,0) circle[x radius=3cm, y radius=3cm];
            \node at (0, -4.5) {$\ell_2$};
        \end{tikzpicture}

        \begin{tikzpicture}[scale=0.5]
            \draw [<->] (-4,0)--(4,0);
            \draw [<->] (0,-4)--(0,4);
            \draw [blue] (-3,-3) rectangle(3, 3);
            \node at (0, -4.5) {$\ell_{\infty}$};
        \end{tikzpicture}

        Each intersection with the x or y axis occurs one unit away from the
        origin.

    \item \text{} \\
        \begin{tabular} {l | c c c}
            Distance metric & \ell_1 & \ell_2 & \ell_{\infty} \\ \hline
            $d_{min}$ & black-blue & black-blue & red-blue \\
            $d_{max}$ & black-blue & red-blue & red-blue \\
            $d_{avg}$ & black-blue & red-blue & red-blue \\
            $d_{centroid}$ & black-blue & red-blue & red-blue \\
        \end{tabular}
    \item \text{} \\
        \begin{tikzpicture}[sloped]
            \node (a) at (0,0) {};
            \node (b) at (-2,0) {};
            \node (c) at (-4,0) {};
            \node (d) at (-6,0) {};
            \node (ab) at (-1,2) {};
            \node (bc) at (-3,4) {};
            \node (cd) at (-5,6) {};
            \node (dcb) at (-2.5, 4) {};
            \node (label) at (-3, 6.3) {$\text{Using } d_{min}:$};

            \draw  (a) |- (ab.center);
            \draw  (b) |- (ab.center);
            \draw  (c) |- (bc.center);
            \draw  (ab.center) |- (bc.center);
            \draw  (d) |- (cd.center);
            \draw  (dcb.center) |- (cd.center);

            \filldraw[fill=black] (a) circle[x radius=0.4, y radius=0.4];
            \filldraw[fill=black] (b) circle[x radius=0.4, y radius=0.4];
            \filldraw[fill=blue] (c) circle[x radius=0.4, y radius=0.4];
            \filldraw[fill=red] (d) circle[x radius=0.4, y radius=0.4];

            \draw[->,-triangle 60] (-7,0) -- node[above]{distance} (-7,6);
        \end{tikzpicture}

        \begin{tikzpicture}[sloped]
            \node (a) at (0,0) {black};
            \node (b) at (-2,0) {black};
            \node (c) at (-4,0) {blue};
            \node (d) at (-6,0) {red};
            \node (ab) at (-1,2) {};
            \node (cd) at (-5,3) {};
            \node (abcd) at (-3,5) {};
            \node (label) at (-3, 6.3) {$\text{Using } d_{max}, d_{avg},
                d_{centroid}:$};

            \draw  (a) |- (ab.center);
            \draw  (b) |- (ab.center);
            \draw  (c) |- (cd.center);
            \draw  (d) |- (cd.center);
            \draw  (ab.center) |- (abcd.center);
            \draw  (cd.center) |- (abcd.center);

            \filldraw[fill=black] (a) circle[x radius=0.4, y radius=0.4];
            \filldraw[fill=black] (b) circle[x radius=0.4, y radius=0.4];
            \filldraw[fill=blue] (c) circle[x radius=0.4, y radius=0.4];
            \filldraw[fill=red] (d) circle[x radius=0.4, y radius=0.4];

            \draw[->,-triangle 60] (-7,0) -- node[above]{distance} (-7,6);
        \end{tikzpicture}

\end{enumerate}

\newpage

\section*{Expectation-Maximization for Gaussian Mixture Models [7pts]}


In this problem we will explore expectation-maximization for the
Gaussian Mixture model.  Each observation $\boldx_i$ is a vector in
$\mathbb{R}^{D}$.  We posit that each observation comes from
\emph{one} mixture component.  For this problem, we will assume there
are $c$~components. Each component $k \in \{1, \ldots, c\}$ will be
associated with a mean vector $\mu_k \in R^{D}$ and a covariance
$\Sigma_k$.  Finally let the (unknown) overall mixing proportion of
the components be~$\btheta \in [0,1]^c$, where~${\sum_{k=1}^c
  \theta_k=1}$.

Our generative model is that each of the~$n$ observations comes from a
single component.  We encode observation $i$'s component-assignment as
a one-hot vector~${\boldz_i \in \{0,1\}^c}$ over components. This
one-hot vector is drawn from~$\btheta$; then, $\boldx_i$ is drawn
from~$N(\mu_{z_i}, \Sigma_{z_i})$. Formally documents are generated in two steps:
\begin{eqnarray*}
 \boldz_i &\sim& \text{Categorical}(\btheta) \\
 \boldx_i &\sim& N(\mu_{z_i}, \Sigma_{z_i})
\end{eqnarray*}


\begin{problem}
  ~

  \begin{enumerate}

    % FDV mostly at Mark Goldstein, delete once read: changed to be
    % intractability of the data likelihood because we don't have
    % priors over the mus and Sigmas in this problem.  We could change
    % the whole problem to add priors, but that might be too hard/a
    % better problem for 281...
  \item \textbf{Intractability of the Data Likelihood} Let $\phi_k$
    represent all the parameters associated with a component
    $(\mu_k,\Sigma_k)$.  We are generally interested in finding a set
    of parameters $\phi_k$ that maximize the data likelihood $\log
    p(\{x_i\}|\{phi_k\})$.  Expand the data likelihood to include the
    necessary sums over observations $x_i$ and latents $z_i$.  Why is
    optimizing this loss directly intractable?

\item \textbf{Complete-Data Log Likelihood} Define the complete data for this
    problem to be $D = \{(\boldx_i, \boldz_i)\}_{i=1}^n$. Write out the
    complete-data (negative) log likelihood. \[\mcL(\btheta,
        \{\mu_k,\Sigma_k\}^c_{k=1}) =  -\ln p(D \given\btheta,
    \{\mu_k,\Sigma_k\}^c_{k=1}).\]


\item \textbf{Expectation Step} Our next step is to introduce a mathematical
    expression for $\boldq_i$, the posterior over the hidden topic
    variables~$\boldz_i$ conditioned on the observed data $\boldx_i$ with fixed
    parameters, i.e $p(\boldz_i | \boldx_i; \btheta, \{ \mu_k,\Sigma_k
    \}^c_{k=1})$.

\begin{itemize}
    \item  Write down and simplify the expression for $\boldq_i$.
    \item  Give an algorithm for calculating $\boldq_i$ for all $i$, given the
        observed data~$\{\boldx_i\}^n_{i=1}$ and settings of the
        parameters~$\btheta$ and~$\{ \mu_k,\Sigma_k  \}^c_{k=1}$.

\end{itemize}

\item \textbf{Maximization Step}
    Using the~$\boldq_i$ estimates from the Expectation Step, derive an update
    for maximizing the expected complete data log likelihood in terms
    of~$\btheta$ and~$\{ \mu_k,\Sigma_k \}^c_{k=1}$.

\begin{itemize}
    \item Derive an expression for the expected complete-data log likelihood in
        terms of $\boldq_i$.
    \item Find an expression for $\btheta$ that maximizes this expected
        complete-data log likelihood. You may find it helpful to use Lagrange
        multipliers in order to force the constraint $\sum \theta_k = 1$. Why
        does this optimized $\btheta$ make intuitive sense?
    \item Apply a similar argument to find the value of the
        $(\mu_k,\Sigma_k)$'s that maximizes the expected complete-data log
        likelihood.
\end{itemize}

\item Finally, compare this EM approach to the generative model for
    classification in Homework 2.  How are the computations similar?
    Different?

\end{enumerate}

\end{problem}

\subsection*{Solution}
\begin{enumerate}
    \item
        \begin{align*}
            \log p(x|\phi) &= \log \sum_{z}p(x,z|\phi) \\
                           &= \sum_{n=1}^{N}\log\sum_{z}p(x_n,z_n|\phi) \\
                           &= \sum_{n=1}^{N}\log\sum_{z}p(x_n,z_n|\phi) + \log
                              p(z_n|\phi) \\
                           &= \sum_{n=1}^{N}N(x_n|\mu_{z_n},\Sigma_{z_n}) +
                              \log \sum_{z} \text{Cat}(z_n|\phi) \\
                           &= \sum_{n=1}^{N}N(x_n|\mu_{z_n},\Sigma_{z_n}) +
                              \log \sum_{z} \theta_{z_n}
        \end{align*}
        This is intractable because we have a sum inside the log, therefore
        there is no closed expression for the MLE.
    \item
        \begin{align*}
            \log p(x,z|\phi) &= \sum_{n=1}^{N}\log p(x_n,z_n|\phi) \\
                             &= \sum_{n=1}^{N}\log p(x_n,z_n|\phi) + \log
                                p(z_n|\phi) \\
                             &= \sum_{n=1}^{N}\log N(x_n|\mu_{z_n},
                                \Sigma_{z_n}) + \log \text{Cat}(z_n|\phi) \\
                             &= \sum_{n=1}^{N}\log N(x_n|\mu_{z_n},
                                \Sigma_{z_n}) + \log \theta_{z_n} \\
                             &= \sum_{n=1}^{N}\sum_{k=1}^{c}z_{n,k} \log
                                N(x_n|\mu_k,\Sigma_k) + z_{n,k} \log \theta_k
        \end{align*}
    \item
        \begin{align*}
            q_i &= p(z_i|x_i,\mu,\Sigma,\theta) \\
                &\propto p(x_i|z_i,\mu,\Sigma,\theta)
                    p(z_i|\mu,\Sigma,\theta) \\
                &= N(x_i|\mu_{z_i},\Sigma_{z_i})\theta
        \end{align*}
        Therefore, to find the probability $q_{i,k}$ (the probability that
        datapoint $i$ comes from cluster $k$, we multiply the given prior
        $\theta_k$, times the liklihood of the point being in that Gaussian
        cluster $N(x_i|\mu_k,\Sigma_k)$). We can look over the data
        algorithmically and do these calculations easily.
    \item
        \begin{align*}
            E_z[\mcL_{\text{complete}}(\phi)]
                &= E_z[\log p(x_n,z_n|\phi)] \\
                &= E_z \bigg[\sum_{n=1}^N\sum_{k=1}^{c} z_{n,k} \log\theta_k +
                   z_{n,k} \log N(x_n|\mu_k,\Sigma_k) \bigg] \\
                &= \sum_{n=1}^N\sum_{k=1}^c (q_{n,k} \log\theta_k + q_{n,k}
                   \log N(x_n|\mu_k,\Sigma_k)) \\
                &\to \sum_{n=1}^N\sum_{k=1}^c (q_{n,k} \log\theta_k + q_{n,k}
                   \log N(x_n|\mu_k,\Sigma_k)) - \lambda \bigg(\sum_k
                   \theta_k-1 \bigg) \\
            \frac{dE_z}{d\phi_k}
                &= \sum_{n=1}^{N}\frac{q_{n,k}}{\theta_k} - \lambda = 0 \\
            \frac{dE_z}{d\lambda}
                &= \sum_{n=1}^{N}\theta_k = 1 \\
            \lambda
                &= \frac{1}{c} \sum_{n=1}^{N} \sum k =
                   1^c \frac{q_{n,k}}{\theta_k} = N \\
            \hat{\theta}_k
            &= \frac{\sum_n q_{n,k}}{N}
        \end{align*}
    \item
        The MLEs from T2 are similar to these. Instead of averaging over
        supervised classifications in T2, we average over $q_{n,k}$ from the
        E-step.
\end{enumerate}

\newpage

\section*{K-Means [15 pts]} % FDV: Any more interesting data sets?

For this problem you will implement  K-Means clustering from scratch. Using
\texttt{numpy} is fine, but don't use a third-party machine learning
implementation like \texttt{scikit-learn}. You will then apply this approach to
clustering of image data.



We have provided you with the MNIST dataset, a collection of handwritten digits
used as a benchmark of image recogntion (you  can learn more about the data set
at  \url{http://yann.lecun.com/exdb/mnist/}). The MNIST task is widely used in
supervised learning, and modern algorithms with neural networks do very well on
this task.

Here we will use MNIST unsupervised learning. You have been given
representations of 6000 MNIST images, each of which are $28\times28$ greyscale
handwritten digits. Your job is to implement K-means clustering on MNIST, and
to test whether this relatively simple algorithm can cluster similar-looking
images together.

~

\begin{problem}
The given code loads the images into your environment as a 6000x28x28 array.

\begin{itemize}
\item Implement K-means clustering from different random initializations and
    for several values of $K$ using the $\ell_2$ norm as your distance metric.
    (You should feel free to explore other metrics than the $\ell_2$ norm, but
    this is strictly optional.)  Compare the K-means objective for different
    values of K and across random initializations.

\item For three different values of K, and a couple of random restarts for
    each, show the mean images for each cluster (i.e., for the cluster
    prototypes), as well as the images for a few representative images for each
    cluster. You should explain how you selected these representative images.
    To render an image, use the pyplot \texttt{imshow} function.

\item Are the results wildly different for different restarts and/or different
    values of K?  For one of your runs, plot the K-means objective function as
    a function of iteration and verify that it never increases.

%\item Finally, implement K-means++ and see if gives you more satisfying
%initializations (and final results) for K-means. Explain your findings.

\end{itemize}

As in past problem sets, please include your plots in this document. (There may
be several plots for this problem, so feel free to take up multiple pages.)


\end{problem}
\subsection*{Solution}
\begin{enumerate}
    \item
        The Kmeans objectives vary based on initialization and value of $K$,
        and loss goes down for larger values of $K$.
    \item
        Here are the images for $K = 10$. The conclusion is that higher $K$
        makes centroid images more crisp, and can encapsulate differences in
        writing (such as the cross on the 7). There are some errors, but this
        is the best I could do. \\
        \includegraphics[scale=0.3]{figures/k3_centroids/9.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/2.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/3.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/4.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/6.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/6.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/8.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/0.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/1.png} \\
        \includegraphics[scale=0.3]{figures/k3_centroids/5.png} \\
    \item
        Loss does not increase at any iteration. \\
        \includegraphics[scale=0.8]{figures/loss.png}
\end{enumerate}

% Figure out how to load it into your environment and turn it into a set of
% vectors.  Run K-Means on it for a few different~$K$ and show some results
% from the fit.  What do the mean images look like?  What are some
% representative images from each of the clusters?  Are the results wildly
% different for different restarts and/or different~$K$?  Plot the K-Means
% objective function (distortion measure) as a function of iteration and verify
% that it never increases.

% \subsection*{4. Implement K-Means++ [4 pts]} mplement K-Means++ and see if it
% gives you more satisfying initializations for K-Means.  Explain your
% findings.

\newpage
\begin{problem}[Calibration, 1pt]
Approximately how long did this homework take you to complete?
15 hours
\end{problem}


\end{document}
